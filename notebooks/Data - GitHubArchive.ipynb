{
 "metadata": {
  "name": "",
  "signature": "sha256:d3676fe68ec742ec4ea00d77da0f5cf7ef395999e7c7938d4f20322b01350b2f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "GitHubArchive consumer"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Loading data from GitHub Archive"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We're going to use [GitHubArchive](http://www.githubarchive.com) to retrieve a lot of data from GitHub activity stream.\n",
      "GitHubArchive provides several file whose names are http://data.githubarchive.org/{year}-{month}-{day}-{hour}.json.gz that we are going to retrieve. GitHubArchive provides those files since 2011-12-02, but the file format has changed at the beginning of 2015.\n",
      "\n",
      "We first generate a list of links we are interested in, say from 2013-01-01 to 2013-01-05 excluded."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from dateutil import rrule\n",
      "from datetime import date\n",
      "\n",
      "start_date = date(2013, 1, 1)\n",
      "end_date = date(2013, 1, 5)\n",
      "\n",
      "date_list = rrule.rrule(rrule.HOURLY, dtstart=start_date, until=end_date)\n",
      "\n",
      "link_format = 'http://data.githubarchive.org/{year}-{month:0>2}-{day:0>2}-{hour}.json.gz'\n",
      "\n",
      "links = [link_format.format(year=d.year, month=d.month, day=d.day, hour=d.hour) for d in date_list]\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    print '\\n'.join(links)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The easiest way to retrieve several files is using `wget` (or `requests` module in Python). Assuming you stored the list of links in a file *links.txt*: \n",
      "```\n",
      "wget -i links.txt -nc -c\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Those files are mainly JSON strings that are gzipped. It is easy to get its content using the following function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import gzip\n",
      "\n",
      "def get_content_from_file(filepath):\n",
      "    \"\"\"\n",
      "    Return a list of JSON structures that are contained in the file\n",
      "    described by filepath. This function expects that the file is \n",
      "    gzipped.\n",
      "    \"\"\"\n",
      "    \n",
      "    with gzip.GzipFile(filepath) as f:\n",
      "        try:\n",
      "            content = map(json.loads, f.readlines())\n",
      "        except Exception as e: \n",
      "            # Anything related to JSON, like UnicodeError, ValueError, etc. \n",
      "            pass  # ... for sure, it's a bad idea...\n",
      "    return content\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Say we are interested to get the content of every file that we downloaded:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "\n",
      "# Assuming we are inside the right directory and there is no other files in it.\n",
      "filename_list = os.listdir('.')  \n",
      "\n",
      "# 'activity' will store the entire activity stream\n",
      "activity = []\n",
      "for filename in filename_list: \n",
      "    activity += get_content_from_file(filename)\n",
      "    \n",
      "# Now, activity contains the entiere activity stream\n",
      "print activity[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This content can then be put in a (relational or not) database. An event looks like:\n",
      "```\n",
      "{u'actor': u'lastr2d2',\n",
      " u'actor_attributes': {u'email': u'lastr2d2@gmail.com',\n",
      "  u'gravatar_id': u'2a8a2ef556894cb1b6945a8c471bc4e9',\n",
      "  u'login': u'lastr2d2',\n",
      "  u'name': u'Wayne Wang',\n",
      "  u'type': u'User'},\n",
      " u'created_at': u'2014-01-01T01:01:58-08:00',\n",
      " u'payload': {u'head': u'afa9b3ac304d6ab92fd7689d1604f240b8f4ae38',\n",
      "  u'ref': u'refs/heads/master',\n",
      "  u'shas': [[u'afa9b3ac304d6ab92fd7689d1604f240b8f4ae38',\n",
      "    u'lastr2d2@gmail.com',\n",
      "    u'updated minifized version',\n",
      "    u'Wayne Wang',\n",
      "    True]],\n",
      "  u'size': 1},\n",
      " u'public': True,\n",
      " u'repository': {u'created_at': u'2013-11-19T00:01:51-08:00',\n",
      "  u'description': u'My userscript for douban.fm',\n",
      "  u'fork': False,\n",
      "  u'forks': 0,\n",
      "  u'has_downloads': True,\n",
      "  u'has_issues': True,\n",
      "  u'has_wiki': True,\n",
      "  u'id': 14517966,\n",
      "  u'language': u'JavaScript',\n",
      "  u'master_branch': u'master',\n",
      "  u'name': u'scripts-doubanfm',\n",
      "  u'open_issues': 0,\n",
      "  u'owner': u'lastr2d2',\n",
      "  u'private': False,\n",
      "  u'pushed_at': u'2014-01-01T01:01:57-08:00',\n",
      "  u'size': 128,\n",
      "  u'stargazers': 0,\n",
      "  u'url': u'https://github.com/lastr2d2/scripts-doubanfm',\n",
      "  u'watchers': 0},\n",
      " u'type': u'PushEvent',\n",
      " u'url': u'https://github.com/lastr2d2/scripts-doubanfm/compare/ba4d721b3d...afa9b3ac30'}\n",
      "```\n",
      "\n",
      "Notice that `payload` has not a fixed schema. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Filtering R packages"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We filtered out the events (and by extension their repositories) that are related to R language. This can be easily done by filtering on `repository.language` key. With such a list of R repositories, we are interested in identifying which ones are R packages. To do this, we filtered out the repositories that contains a **DESCRIPTION** file at its root. \n",
      "\n",
      "If *_repository_* is the (full) name of the repository, then this file could possibly be retrieved from https://raw.githubusercontent.com/_repository_/master/DESCRIPTION"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "\n",
      "url = 'https://raw.githubusercontent.com/{repository}/master/DESCRIPTION'\n",
      "\n",
      "def get_description_file_for(repository):\n",
      "    \"\"\" \n",
      "    Given the full name of a github repository, return the DESCRIPTION file\n",
      "    if it exists or None otherwhise. \n",
      "    \"\"\"\n",
      "    \n",
      "    result = requests.get(url.format(repository))\n",
      "    if status_code == 200:\n",
      "        return r.content\n",
      "    else:\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We put all the data collected from 2013 and 2014 in a MongoDB datastore. Our MongoDB contains a collection `events` with every event from GitHub Archive related to R. We spread the data into several collections: `events` contains the raw event, `repository` contains `event.repository`, `payload` contains `event.payload`, etc. for every subdocument contained in each `event`. \n",
      "\n",
      "Here's the result of some queries, FYI:\n",
      "```\n",
      "> db.events.count()\n",
      "1016423\n",
      "> db.events.distinct('repository.url').length\n",
      "121385\n",
      "> db.events.distinct('repository.id').length\n",
      "118675\n",
      "> db.events.distinct('repository.name').length\n",
      "43164\n",
      "```\n",
      "\n",
      "Notice that, at the same time, https://github.com/search?utf8=%E2%9C%93&q=language%3AR&type=Repositories&ref=searchresults shows 67275 repositories. This can be explained as a large majority of the 121385 repositories we have have been deleted since the events were collected.\n",
      "\n",
      "Moreover, we added a collection `descriptionfile`. This collection contains `{_id: URL, file: CONTENT}` where `URL` is the URL of a R repository, and `CONTENT` is the content of the *DESCRIPTION* file if any. \n",
      "\n",
      "```\n",
      "> db.descriptionfile.find({file: {$exists: true}}).length()\n",
      "19052\n",
      "```\n",
      "\n",
      "This is, we collected and identified 19052 packages that are related to R. This includes redundant repositories, like `rpkg/*` which is an alias for `cran/*`. \n",
      "\n",
      "```\n",
      "> db.descriptionfile.find({_id: { $regex: /^https:\\/\\/github.com\\/cran/ } } ).length()\n",
      "6007\n",
      "> db.descriptionfile.find({_id: { $regex: /^https:\\/\\/github.com\\/rpkg/ } } ).length()\n",
      "4423\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Identifying R packages from CRAN, RPKG and the others"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Say we are interested in identifying which are the R packages that are hosted by CRAN on Github:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymongo\n",
      "\n",
      "# Assuming we are locally running a MongoDB instance\n",
      "db = pymongo.MongoClient().r\n",
      "\n",
      "# Contains 3-uples ('https://github.com', OWNER_NAME, REPOSITORY_NAME)\n",
      "packages = map(lambda x: x['_id'].rsplit('/', 2), list(db.descriptionfile.find({'file': {'$exists': True}}, fields=['_id'])))\n",
      "\n",
      "# Filter CRAN, RPKG and the others\n",
      "cran_names = map(lambda x: x[2], filter(lambda x: x[1] == 'cran', packages))\n",
      "rpkg_names = map(lambda x: x[2], filter(lambda x: x[1] == 'rpkg', packages))\n",
      "other_names = map(lambda x: x[2], filter(lambda x: x[1] != 'rpkg' and x[1] != 'cran', packages))\n",
      "\n",
      "# len(other_names) == 8643\n",
      "\n",
      "cran_set = set(cran_names)\n",
      "rpkg_set = set(rpkg_names)\n",
      "other_set = set(other_names)\n",
      "\n",
      "# Names that are NOT in cran_set and rpkg_set: 5068 items\n",
      "outside_only = other_set.difference(cran_set).difference(rpkg_set)\n",
      "\n",
      "# Names that are in cran_set: 1210 items\n",
      "cran_too = other_set.intersection(cran_set)\n",
      "\n",
      "# Names that are in rpkg_set: 753 items\n",
      "rpkg_too = other_set.intersection(rpkg_set)\n",
      "\n",
      "# Names from cran that are in rpkg too: 4345 items (rpkg has 4410 items!)\n",
      "rpkg_cran = rpkg_set.intersection(cran_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}